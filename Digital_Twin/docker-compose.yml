version: "3.8"

# Define a base configuration for celery services to reduce repetition
x-celery-base: &celery-base
  build:
    context: . # Build context is the Digital_Twin directory
    dockerfile: Dockerfile # Use the main Dockerfile
  restart: unless-stopped
  env_file:
    - ./.env # Load common environment variables (DB URL, Redis URL, etc.)
  environment:
    - FLASK_ENV=production # Ensure Flask runs in production mode
    - CELERY_BROKER_URL=redis://redis:6379/1 # Redis instance for Celery broker (DB 1)
    - CELERY_RESULT_BACKEND=redis://redis:6379/2 # Redis instance for Celery results (DB 2)
  volumes:
    # Mount necessary volumes for data persistence and access
    # Use bind mounts for code/config during development if needed,
    # but the Dockerfile handles copying code for production builds.
    # Keep named volumes for persistent data.
    - ./LOGS:/app/LOGS # Log persistence might be useful
    # Persistent volumes defined below are generally better for DB data etc.
  depends_on:
    redis:
      condition: service_healthy # Wait for Redis to be healthy
    postgres:
      condition: service_healthy # Wait for Postgres to be healthy
  networks:
    - digital_twin_network

services:
  # ----------------------------------------
  # Main Application (Flask + SocketIO)
  # ----------------------------------------
  digital-twin-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: digital_twin_app
    restart: unless-stopped
    expose:
      - "5000" # Expose internally for Nginx proxy
    env_file:
      - ./.env # Load DB URL, secrets, Redis URL, etc.
    environment:
      - FLASK_ENV=production
      - REDIS_URL=redis://redis:6379/0 # Redis for caching and SocketIO message queue (DB 0)
      - DATABASE_URL=postgresql://dt_user:${POSTGRES_PASSWORD:-default_pw}@postgres:5432/digital_twin # Ensure password variable is used
      # ENCRYPTION_KEY should be in .env
    volumes:
      # Named volumes preferred for persistent data
      - postgres_data:/app/DATABASE # Example if DB files were expected here, adjust if needed
      - reports_data:/app/REPORTS # Persistent reports
      - analytics_models:/app/ANALYTICS/models # Persistent models
      - analytics_cache:/app/ANALYTICS/analysis_cache # Persistent cache
      - security_backups:/app/SECURITY/data_backups # Persistent backups
      - logs_data:/app/LOGS # Persistent logs
      # Mount config read-only if it's not generated by the app
      - ./CONFIG:/app/CONFIG:ro
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ----------------------------------------
  # Redis (Caching, Celery, SocketIO MQ)
  # ----------------------------------------
  redis:
    image: redis:7-alpine
    container_name: digital_twin_redis
    restart: unless-stopped
    expose:
      - "6379" # Internal port
    volumes:
      - redis_data:/data # Use named volume for persistence
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ----------------------------------------
  # PostgreSQL Database
  # ----------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: digital_twin_postgres
    restart: unless-stopped
    env_file:
      - ./.env # Load POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
    expose:
      - "5432" # Internal port
    volumes:
      - postgres_data:/var/lib/postgresql/data # Use named volume for persistence
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ----------------------------------------
  # Celery Worker (Background Tasks)
  # ----------------------------------------
  celery-worker:
    <<: *celery-base # Inherit common config
    container_name: digital_twin_celery_worker
    command: celery -A Digital_Twin.WEB_APPLICATION.enhanced_flask_app_v2:celery_app worker --loglevel=info
    healthcheck:
      test: ["CMD", "celery", "-A", "Digital_Twin.WEB_APPLICATION.enhanced_flask_app_v2:celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ----------------------------------------
  # Celery Beat (Scheduled Tasks)
  # ----------------------------------------
  celery-beat:
    <<: *celery-base # Inherit common config
    container_name: digital_twin_celery_beat
    command: celery -A Digital_Twin.WEB_APPLICATION.enhanced_flask_app_v2:celery_app beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler # Or remove scheduler arg for default
    # Healthcheck for Beat is less straightforward, often monitoring logs or a heartbeat file is used.

  # ----------------------------------------
  # MQTT Ingestor (Listens to MQTT Broker)
  # ----------------------------------------
  mqtt_ingestor:
    <<: *celery-base # Reuses build context, env vars, volumes
    container_name: digital_twin_mqtt_ingestor
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      mosquitto: # Depends on MQTT Broker
        condition: service_started # Basic check, healthcheck preferred if available
    command: python -u Digital_Twin/AI_MODULES/mqtt_ingestor.py
    # Healthcheck: Could check if the process is running, or implement a health endpoint/signal

  # ----------------------------------------
  # Nginx (Reverse Proxy & HTTPS)
  # ----------------------------------------
  nginx:
    image: nginx:alpine
    container_name: digital_twin_nginx
    restart: unless-stopped
    ports:
      - "${HTTP_PORT:-80}:80"    # Use env var for flexibility, default to 80
      - "${HTTPS_PORT:-443}:443" # Use env var, default to 443
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro # Ensure your certs are here
    depends_on:
      digital-twin-app:
        condition: service_healthy
      grafana: # Wait for Grafana to start
        condition: service_started # Basic check
      prometheus: # Wait for Prometheus to start
        condition: service_started # Basic check
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD", "nginx", "-t"] # Test Nginx configuration
      interval: 60s
      timeout: 5s
      retries: 3

  # ----------------------------------------
  # Prometheus (Metrics Collection)
  # ----------------------------------------
  prometheus:
    image: prom/prometheus:latest
    container_name: digital_twin_prometheus
    restart: unless-stopped
    expose:
      - "9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus # Use named volume for persistence
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.external-url=http://${DOMAIN_NAME:-localhost}/prometheus/' # Use http if Nginx handles TLS
      - '--web.route-prefix=/'
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ----------------------------------------
  # Grafana (Metrics Visualization)
  # ----------------------------------------
  grafana:
    image: grafana/grafana-oss:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - digital_twin_network
    depends_on:
      - prometheus

  # ----------------------------------------
  # InfluxDB (Time Series Data - Optional)
  # ----------------------------------------
  influxdb:
    image: influxdb:2.7-alpine
    container_name: digital_twin_influxdb
    restart: unless-stopped
    env_file:
      - ./.env
    expose:
      - "8086"
    volumes:
      - influxdb_data:/var/lib/influxdb2 # Use named volume for persistence
    networks:
      - digital_twin_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8086/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ----------------------------------------
  # Mosquitto (MQTT Broker)
  # ----------------------------------------
  mosquitto:
    image: eclipse-mosquitto:latest
    container_name: digital_twin_mqtt
    restart: unless-stopped
    ports:
      - "1883:1883"
      - "9001:9001"
    volumes:
      - ./mosquitto/config:/mosquitto/config:ro
      - mosquitto_data:/mosquitto/data # Use named volume for persistence
      - mosquitto_log:/mosquitto/log    # Use named volume for persistence
    networks:
      - digital_twin_network
    # Basic healthcheck: check if port is open. A more robust check might involve MQTT ping.
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "1883"]
      interval: 30s
      timeout: 10s
      retries: 3

# --- Networks Definition ---
networks:
  digital_twin_network:
    driver: bridge

# --- Volumes Definition ---
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  influxdb_data:
    driver: local
  mosquitto_data:
    driver: local
  mosquitto_log:
    driver: local
  # Adding volumes mentioned in the app service for clarity/persistence
  reports_data:
    driver: local
  analytics_models:
    driver: local
  analytics_cache:
    driver: local
  security_backups:
    driver: local
  logs_data:
    driver: local